[project]
name = "nocp"
version = "0.1.0"
description = "High-Efficiency LLM Proxy Agent with Token Optimization (TOON Layer)"
authors = [
    {name = "Raudbjorn", email = "info@example.com"}
]
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}

dependencies = [
    "pydantic>=2.0.0",
    "google-generativeai>=0.3.0",
    "python-dotenv>=1.0.0",
    "structlog>=23.1.0",
    "tenacity>=8.2.0",
    "tiktoken>=0.5.0",
    "litellm>=1.55.0",
    "pydantic-settings>=2.0.0",
    "typer>=0.9.0",
    "rich>=13.0.0",
    "tomli>=2.0.0; python_version < '3.11'",
]

[project.scripts]
nocp = "nocp.cli:main"

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.1.0",
    "black>=23.7.0",
    "ruff>=0.0.285",
    "mypy>=1.5.0",
]

litellm = [
    "litellm>=1.0.0",
]

[build-system]
requires = ["setuptools>=68.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.black]
line-length = 100
target-version = ['py310']

[tool.ruff]
line-length = 100
select = ["E", "F", "I", "N", "W", "UP"]
ignore = ["E501"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true

# NOCP Configuration Defaults
# Uncomment and customize the settings below to configure project-specific defaults
# [tool.nocp]
# # Project-specific NOCP defaults
# default_compression_threshold = 5000
# enable_semantic_pruning = true
# enable_knowledge_distillation = false
# default_output_format = "toon"
# log_level = "INFO"
#
# # LLM settings
# litellm_default_model = "gemini/gemini-2.0-flash-exp"
# litellm_fallback_models = "gemini/gemini-1.5-flash,openai/gpt-4o-mini"
#
# # Metrics
# enable_metrics_logging = true
# metrics_log_file = ".nocp/metrics.jsonl"
